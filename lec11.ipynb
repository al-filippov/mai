{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc21e62b",
   "metadata": {},
   "source": [
    "## Глубокое обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e007fa",
   "metadata": {},
   "source": [
    "#### Инициализация Keras\n",
    "\n",
    "В качестве бэкенда используется jax\n",
    "\n",
    "Бэкенд должен указываться до импорта keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88eddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "import keras\n",
    "\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e725c3e2",
   "metadata": {},
   "source": [
    "### Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2018b",
   "metadata": {},
   "source": [
    "#### Загрузка набора данных для задачи классификации\n",
    "\n",
    "База данных MNIST (сокращение от \"Modified National Institute of Standards and Technology\") — объёмная база данных образцов рукописного написания цифр. База данных является стандартом, предложенным Национальным институтом стандартов и технологий США с целью обучения и сопоставления методов распознавания изображений с помощью машинного обучения в первую очередь на основе нейронных сетей. Данные состоят из заранее подготовленных примеров изображений, на основе которых проводится обучение и тестирование систем.\n",
    "\n",
    "База данных MNIST содержит 60000 изображений для обучения и 10000 изображений для тестирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7a770c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.api.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_valid, y_valid) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2231ce2",
   "metadata": {},
   "source": [
    "#### Отображение данных\n",
    "\n",
    "Образцы из набора прошли сглаживание и приведены к серому полутоновому изображению размером 28x28 пикселей.\n",
    "\n",
    "Под каждым изображением представлено соответствующее ему значение целевого признака (класс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cfd2770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from turtle import width\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(5, 5))\n",
    "for k in range(12):\n",
    "    current_axes = axes[k % 3][k % 4]\n",
    "    current_axes.imshow(X_train[k], cmap='grey')\n",
    "    current_axes.get_xaxis().set_ticks([])\n",
    "    current_axes.get_yaxis().set_ticks([])\n",
    "    current_axes.set_xlabel(y_train[k])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3639caf",
   "metadata": {},
   "source": [
    "#### Предобработка данных\n",
    "\n",
    "Количество классов - 10 (от 0 до 9).\n",
    "\n",
    "Все изображения из X трансформируются в векторы длиной 784 (28*28) признака и нормализуются.\n",
    "\n",
    "Для целевых признаков применяется унитарное кодирование в бинарные векторы длиной 10 (нормализация)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a696e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "       0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117648, 0.36862746, 0.6039216 ,\n",
       "       0.6666667 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235295, 0.6745098 , 0.99215686, 0.9490196 ,\n",
       "       0.7647059 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215687, 0.93333334,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.9843137 , 0.3647059 ,\n",
       "       0.32156864, 0.32156864, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882354, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.7137255 ,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.3137255 , 0.6117647 , 0.41960785, 0.99215686, 0.99215686,\n",
       "       0.8039216 , 0.04313726, 0.        , 0.16862746, 0.6039216 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509807,\n",
       "       0.99215686, 0.74509805, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313726, 0.74509805, 0.99215686,\n",
       "       0.27450982, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.13725491, 0.94509804, 0.88235295, 0.627451  ,\n",
       "       0.42352942, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764707, 0.9411765 , 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "       0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.3647059 ,\n",
       "       0.9882353 , 0.99215686, 0.73333335, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.9764706 , 0.99215686,\n",
       "       0.9764706 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980395,\n",
       "       0.7176471 , 0.99215686, 0.99215686, 0.8117647 , 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.5803922 , 0.8980392 , 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.7137255 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882354, 0.8352941 , 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.31764707,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058825, 0.85882354,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7647059 ,\n",
       "       0.3137255 , 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.6745098 ,\n",
       "       0.8862745 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156866, 0.04313726, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333336, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137256, 0.5294118 , 0.5176471 , 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_classes = 10\n",
    "\n",
    "X_train = X_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "X_valid = X_valid.reshape(10000, 784).astype(\"float32\") / 255\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_valid = keras.utils.to_categorical(y_valid, n_classes)\n",
    "\n",
    "display(X_train[0])\n",
    "display(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40173015",
   "metadata": {},
   "source": [
    "#### Проектирование архитектуры простой ИНС\n",
    "\n",
    "Сеть состоит из:\n",
    "- входного слоя с 784 входами (InputLayer);\n",
    "- скрытого полносвязного слоя с 64 sigmoid-нейронами (dense_2);\n",
    "- выходного слоя с 10 softmax-нейронами (многоклассовая классификация) (dense_3).\n",
    "\n",
    "Количество параметров в слоях:\n",
    "- dense_2: 784 * 64 + 64 = 50 176 + 64 = 50 240. У каждого из 64 нейронов 784 входа с 784 параметрами (w * x) + 64 смещения (b).\n",
    "- dense_3: 64 * 10 + 10 = 640 + 10 = 650.\n",
    "\n",
    "Всего параметров: 50 240 + 650 = 50 890.\n",
    "\n",
    "Все параметры настраиваются в процессе обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91b073d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,890</span> (198.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,890\u001b[0m (198.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,890</span> (198.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,890\u001b[0m (198.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import Dense, InputLayer\n",
    "\n",
    "simple_model = Sequential()\n",
    "simple_model.add(InputLayer(shape=(28*28,)))\n",
    "simple_model.add(Dense(64, activation=\"sigmoid\"))\n",
    "simple_model.add(Dense(n_classes, activation=\"softmax\"))\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a21eb9e",
   "metadata": {},
   "source": [
    "#### Обучение простой модели\n",
    "\n",
    "Функция стоимости: MSE (квадратичная функция)\n",
    "\n",
    "Оптимизатор: стохастический градиентный спуск (SGD)\n",
    "\n",
    "Скорость обучения: 0.01\n",
    "\n",
    "Количество эпох: 200\n",
    "\n",
    "Размер пакета: 128\n",
    "\n",
    "Всего пакетов: 60 000 / 128 = 468.75 (468 пакетов по 128 изображений и 1 пакет с 96 изображениями)\n",
    "\n",
    "Метрика оценки качества: accuracy\n",
    "\n",
    "Оценка качества и стоимость на обучающей выборке:\\\n",
    "accuracy: 0.4650 - loss: 0.0849\n",
    "\n",
    "Оценка качества и стоимость на тестовой выборке:\\\n",
    "val_accuracy: 0.4703 - val_loss: 0.0845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b65eca38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4592 - loss: 0.0852 - val_accuracy: 0.4660 - val_loss: 0.0849\n",
      "Epoch 2/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717us/step - accuracy: 0.4650 - loss: 0.0849 - val_accuracy: 0.4703 - val_loss: 0.0845\n",
      "Epoch 3/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step - accuracy: 0.4693 - loss: 0.0845 - val_accuracy: 0.4727 - val_loss: 0.0842\n",
      "Epoch 4/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - accuracy: 0.4708 - loss: 0.0842 - val_accuracy: 0.4742 - val_loss: 0.0838\n",
      "Epoch 5/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - accuracy: 0.4721 - loss: 0.0839 - val_accuracy: 0.4761 - val_loss: 0.0834\n",
      "Epoch 6/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - accuracy: 0.4713 - loss: 0.0835 - val_accuracy: 0.4774 - val_loss: 0.0831\n",
      "Epoch 7/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - accuracy: 0.4721 - loss: 0.0831 - val_accuracy: 0.4799 - val_loss: 0.0827\n",
      "Epoch 8/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - accuracy: 0.4748 - loss: 0.0827 - val_accuracy: 0.4814 - val_loss: 0.0823\n",
      "Epoch 9/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - accuracy: 0.4782 - loss: 0.0823 - val_accuracy: 0.4837 - val_loss: 0.0819\n",
      "Epoch 10/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - accuracy: 0.4817 - loss: 0.0819 - val_accuracy: 0.4848 - val_loss: 0.0815\n",
      "Epoch 11/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - accuracy: 0.4838 - loss: 0.0815 - val_accuracy: 0.4872 - val_loss: 0.0810\n",
      "Epoch 12/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - accuracy: 0.4815 - loss: 0.0811 - val_accuracy: 0.4896 - val_loss: 0.0806\n",
      "Epoch 13/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - accuracy: 0.4834 - loss: 0.0807 - val_accuracy: 0.4923 - val_loss: 0.0802\n",
      "Epoch 14/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - accuracy: 0.4867 - loss: 0.0802 - val_accuracy: 0.4944 - val_loss: 0.0797\n",
      "Epoch 15/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - accuracy: 0.4922 - loss: 0.0798 - val_accuracy: 0.4983 - val_loss: 0.0793\n",
      "Epoch 16/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - accuracy: 0.4941 - loss: 0.0793 - val_accuracy: 0.5021 - val_loss: 0.0788\n",
      "Epoch 17/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - accuracy: 0.4958 - loss: 0.0789 - val_accuracy: 0.5052 - val_loss: 0.0783\n",
      "Epoch 18/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - accuracy: 0.4996 - loss: 0.0785 - val_accuracy: 0.5087 - val_loss: 0.0778\n",
      "Epoch 19/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - accuracy: 0.5028 - loss: 0.0779 - val_accuracy: 0.5114 - val_loss: 0.0773\n",
      "Epoch 20/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - accuracy: 0.5088 - loss: 0.0774 - val_accuracy: 0.5154 - val_loss: 0.0768\n",
      "Epoch 21/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - accuracy: 0.5137 - loss: 0.0769 - val_accuracy: 0.5193 - val_loss: 0.0763\n",
      "Epoch 22/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - accuracy: 0.5170 - loss: 0.0765 - val_accuracy: 0.5229 - val_loss: 0.0758\n",
      "Epoch 23/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - accuracy: 0.5174 - loss: 0.0760 - val_accuracy: 0.5268 - val_loss: 0.0753\n",
      "Epoch 24/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - accuracy: 0.5242 - loss: 0.0755 - val_accuracy: 0.5326 - val_loss: 0.0748\n",
      "Epoch 25/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - accuracy: 0.5264 - loss: 0.0750 - val_accuracy: 0.5385 - val_loss: 0.0743\n",
      "Epoch 26/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - accuracy: 0.5375 - loss: 0.0743 - val_accuracy: 0.5433 - val_loss: 0.0738\n",
      "Epoch 27/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - accuracy: 0.5386 - loss: 0.0739 - val_accuracy: 0.5465 - val_loss: 0.0732\n",
      "Epoch 28/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step - accuracy: 0.5468 - loss: 0.0733 - val_accuracy: 0.5526 - val_loss: 0.0727\n",
      "Epoch 29/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - accuracy: 0.5494 - loss: 0.0729 - val_accuracy: 0.5566 - val_loss: 0.0722\n",
      "Epoch 30/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - accuracy: 0.5534 - loss: 0.0724 - val_accuracy: 0.5613 - val_loss: 0.0717\n",
      "Epoch 31/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - accuracy: 0.5547 - loss: 0.0720 - val_accuracy: 0.5645 - val_loss: 0.0711\n",
      "Epoch 32/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - accuracy: 0.5653 - loss: 0.0712 - val_accuracy: 0.5698 - val_loss: 0.0706\n",
      "Epoch 33/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - accuracy: 0.5675 - loss: 0.0710 - val_accuracy: 0.5730 - val_loss: 0.0700\n",
      "Epoch 34/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - accuracy: 0.5707 - loss: 0.0704 - val_accuracy: 0.5770 - val_loss: 0.0695\n",
      "Epoch 35/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - accuracy: 0.5750 - loss: 0.0698 - val_accuracy: 0.5803 - val_loss: 0.0690\n",
      "Epoch 36/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - accuracy: 0.5782 - loss: 0.0692 - val_accuracy: 0.5837 - val_loss: 0.0684\n",
      "Epoch 37/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - accuracy: 0.5848 - loss: 0.0687 - val_accuracy: 0.5869 - val_loss: 0.0679\n",
      "Epoch 38/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - accuracy: 0.5910 - loss: 0.0681 - val_accuracy: 0.5910 - val_loss: 0.0674\n",
      "Epoch 39/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - accuracy: 0.5947 - loss: 0.0675 - val_accuracy: 0.5959 - val_loss: 0.0669\n",
      "Epoch 40/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968us/step - accuracy: 0.5993 - loss: 0.0669 - val_accuracy: 0.6002 - val_loss: 0.0663\n",
      "Epoch 41/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - accuracy: 0.6002 - loss: 0.0666 - val_accuracy: 0.6038 - val_loss: 0.0658\n",
      "Epoch 42/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - accuracy: 0.6063 - loss: 0.0660 - val_accuracy: 0.6070 - val_loss: 0.0653\n",
      "Epoch 43/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 0.6063 - loss: 0.0656 - val_accuracy: 0.6112 - val_loss: 0.0648\n",
      "Epoch 44/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6083 - loss: 0.0652 - val_accuracy: 0.6143 - val_loss: 0.0642\n",
      "Epoch 45/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - accuracy: 0.6162 - loss: 0.0645 - val_accuracy: 0.6177 - val_loss: 0.0637\n",
      "Epoch 46/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - accuracy: 0.6178 - loss: 0.0640 - val_accuracy: 0.6198 - val_loss: 0.0632\n",
      "Epoch 47/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - accuracy: 0.6199 - loss: 0.0636 - val_accuracy: 0.6231 - val_loss: 0.0627\n",
      "Epoch 48/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - accuracy: 0.6250 - loss: 0.0630 - val_accuracy: 0.6259 - val_loss: 0.0622\n",
      "Epoch 49/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - accuracy: 0.6293 - loss: 0.0624 - val_accuracy: 0.6290 - val_loss: 0.0617\n",
      "Epoch 50/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - accuracy: 0.6317 - loss: 0.0620 - val_accuracy: 0.6321 - val_loss: 0.0612\n",
      "Epoch 51/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - accuracy: 0.6301 - loss: 0.0617 - val_accuracy: 0.6354 - val_loss: 0.0607\n",
      "Epoch 52/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - accuracy: 0.6387 - loss: 0.0609 - val_accuracy: 0.6383 - val_loss: 0.0602\n",
      "Epoch 53/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - accuracy: 0.6368 - loss: 0.0607 - val_accuracy: 0.6408 - val_loss: 0.0598\n",
      "Epoch 54/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - accuracy: 0.6419 - loss: 0.0602 - val_accuracy: 0.6430 - val_loss: 0.0593\n",
      "Epoch 55/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - accuracy: 0.6416 - loss: 0.0597 - val_accuracy: 0.6462 - val_loss: 0.0588\n",
      "Epoch 56/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - accuracy: 0.6457 - loss: 0.0592 - val_accuracy: 0.6483 - val_loss: 0.0583\n",
      "Epoch 57/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - accuracy: 0.6487 - loss: 0.0587 - val_accuracy: 0.6506 - val_loss: 0.0579\n",
      "Epoch 58/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - accuracy: 0.6494 - loss: 0.0582 - val_accuracy: 0.6535 - val_loss: 0.0574\n",
      "Epoch 59/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - accuracy: 0.6557 - loss: 0.0578 - val_accuracy: 0.6569 - val_loss: 0.0570\n",
      "Epoch 60/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - accuracy: 0.6574 - loss: 0.0574 - val_accuracy: 0.6598 - val_loss: 0.0565\n",
      "Epoch 61/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - accuracy: 0.6597 - loss: 0.0569 - val_accuracy: 0.6620 - val_loss: 0.0561\n",
      "Epoch 62/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - accuracy: 0.6587 - loss: 0.0567 - val_accuracy: 0.6638 - val_loss: 0.0556\n",
      "Epoch 63/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.6665 - loss: 0.0559 - val_accuracy: 0.6661 - val_loss: 0.0552\n",
      "Epoch 64/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - accuracy: 0.6635 - loss: 0.0558 - val_accuracy: 0.6684 - val_loss: 0.0548\n",
      "Epoch 65/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - accuracy: 0.6701 - loss: 0.0551 - val_accuracy: 0.6714 - val_loss: 0.0543\n",
      "Epoch 66/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - accuracy: 0.6749 - loss: 0.0546 - val_accuracy: 0.6744 - val_loss: 0.0539\n",
      "Epoch 67/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - accuracy: 0.6762 - loss: 0.0542 - val_accuracy: 0.6771 - val_loss: 0.0535\n",
      "Epoch 68/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - accuracy: 0.6756 - loss: 0.0539 - val_accuracy: 0.6801 - val_loss: 0.0531\n",
      "Epoch 69/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - accuracy: 0.6776 - loss: 0.0535 - val_accuracy: 0.6825 - val_loss: 0.0527\n",
      "Epoch 70/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - accuracy: 0.6819 - loss: 0.0532 - val_accuracy: 0.6857 - val_loss: 0.0523\n",
      "Epoch 71/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - accuracy: 0.6835 - loss: 0.0528 - val_accuracy: 0.6889 - val_loss: 0.0519\n",
      "Epoch 72/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - accuracy: 0.6846 - loss: 0.0523 - val_accuracy: 0.6912 - val_loss: 0.0515\n",
      "Epoch 73/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - accuracy: 0.6873 - loss: 0.0520 - val_accuracy: 0.6941 - val_loss: 0.0511\n",
      "Epoch 74/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - accuracy: 0.6880 - loss: 0.0518 - val_accuracy: 0.6968 - val_loss: 0.0507\n",
      "Epoch 75/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - accuracy: 0.6918 - loss: 0.0513 - val_accuracy: 0.7003 - val_loss: 0.0504\n",
      "Epoch 76/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - accuracy: 0.6940 - loss: 0.0508 - val_accuracy: 0.7029 - val_loss: 0.0500\n",
      "Epoch 77/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - accuracy: 0.6964 - loss: 0.0506 - val_accuracy: 0.7054 - val_loss: 0.0496\n",
      "Epoch 78/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - accuracy: 0.7000 - loss: 0.0503 - val_accuracy: 0.7079 - val_loss: 0.0493\n",
      "Epoch 79/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - accuracy: 0.7057 - loss: 0.0498 - val_accuracy: 0.7101 - val_loss: 0.0489\n",
      "Epoch 80/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - accuracy: 0.7056 - loss: 0.0495 - val_accuracy: 0.7122 - val_loss: 0.0486\n",
      "Epoch 81/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - accuracy: 0.7098 - loss: 0.0492 - val_accuracy: 0.7153 - val_loss: 0.0482\n",
      "Epoch 82/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - accuracy: 0.7116 - loss: 0.0489 - val_accuracy: 0.7187 - val_loss: 0.0479\n",
      "Epoch 83/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - accuracy: 0.7174 - loss: 0.0485 - val_accuracy: 0.7218 - val_loss: 0.0475\n",
      "Epoch 84/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - accuracy: 0.7204 - loss: 0.0480 - val_accuracy: 0.7252 - val_loss: 0.0472\n",
      "Epoch 85/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - accuracy: 0.7229 - loss: 0.0476 - val_accuracy: 0.7286 - val_loss: 0.0469\n",
      "Epoch 86/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - accuracy: 0.7260 - loss: 0.0474 - val_accuracy: 0.7314 - val_loss: 0.0465\n",
      "Epoch 87/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 847us/step - accuracy: 0.7300 - loss: 0.0469 - val_accuracy: 0.7362 - val_loss: 0.0462\n",
      "Epoch 88/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894us/step - accuracy: 0.7317 - loss: 0.0468 - val_accuracy: 0.7391 - val_loss: 0.0459\n",
      "Epoch 89/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - accuracy: 0.7321 - loss: 0.0466 - val_accuracy: 0.7414 - val_loss: 0.0456\n",
      "Epoch 90/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - accuracy: 0.7383 - loss: 0.0459 - val_accuracy: 0.7439 - val_loss: 0.0453\n",
      "Epoch 91/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - accuracy: 0.7392 - loss: 0.0459 - val_accuracy: 0.7469 - val_loss: 0.0450\n",
      "Epoch 92/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - accuracy: 0.7425 - loss: 0.0456 - val_accuracy: 0.7488 - val_loss: 0.0447\n",
      "Epoch 93/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850us/step - accuracy: 0.7444 - loss: 0.0452 - val_accuracy: 0.7514 - val_loss: 0.0444\n",
      "Epoch 94/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - accuracy: 0.7488 - loss: 0.0448 - val_accuracy: 0.7533 - val_loss: 0.0441\n",
      "Epoch 95/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - accuracy: 0.7477 - loss: 0.0448 - val_accuracy: 0.7562 - val_loss: 0.0438\n",
      "Epoch 96/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - accuracy: 0.7525 - loss: 0.0444 - val_accuracy: 0.7578 - val_loss: 0.0435\n",
      "Epoch 97/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - accuracy: 0.7546 - loss: 0.0441 - val_accuracy: 0.7606 - val_loss: 0.0433\n",
      "Epoch 98/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 0.7537 - loss: 0.0439 - val_accuracy: 0.7629 - val_loss: 0.0430\n",
      "Epoch 99/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - accuracy: 0.7606 - loss: 0.0433 - val_accuracy: 0.7644 - val_loss: 0.0427\n",
      "Epoch 100/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - accuracy: 0.7579 - loss: 0.0434 - val_accuracy: 0.7665 - val_loss: 0.0424\n",
      "Epoch 101/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - accuracy: 0.7582 - loss: 0.0433 - val_accuracy: 0.7684 - val_loss: 0.0422\n",
      "Epoch 102/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - accuracy: 0.7633 - loss: 0.0427 - val_accuracy: 0.7713 - val_loss: 0.0419\n",
      "Epoch 103/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.7631 - loss: 0.0426 - val_accuracy: 0.7734 - val_loss: 0.0417\n",
      "Epoch 104/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - accuracy: 0.7657 - loss: 0.0422 - val_accuracy: 0.7752 - val_loss: 0.0414\n",
      "Epoch 105/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - accuracy: 0.7700 - loss: 0.0420 - val_accuracy: 0.7764 - val_loss: 0.0412\n",
      "Epoch 106/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - accuracy: 0.7689 - loss: 0.0419 - val_accuracy: 0.7780 - val_loss: 0.0409\n",
      "Epoch 107/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.7700 - loss: 0.0416 - val_accuracy: 0.7795 - val_loss: 0.0407\n",
      "Epoch 108/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - accuracy: 0.7698 - loss: 0.0414 - val_accuracy: 0.7807 - val_loss: 0.0404\n",
      "Epoch 109/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - accuracy: 0.7738 - loss: 0.0409 - val_accuracy: 0.7818 - val_loss: 0.0402\n",
      "Epoch 110/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - accuracy: 0.7780 - loss: 0.0405 - val_accuracy: 0.7837 - val_loss: 0.0399\n",
      "Epoch 111/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - accuracy: 0.7736 - loss: 0.0405 - val_accuracy: 0.7852 - val_loss: 0.0397\n",
      "Epoch 112/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - accuracy: 0.7745 - loss: 0.0405 - val_accuracy: 0.7860 - val_loss: 0.0395\n",
      "Epoch 113/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - accuracy: 0.7783 - loss: 0.0399 - val_accuracy: 0.7869 - val_loss: 0.0392\n",
      "Epoch 114/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - accuracy: 0.7767 - loss: 0.0400 - val_accuracy: 0.7884 - val_loss: 0.0390\n",
      "Epoch 115/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - accuracy: 0.7798 - loss: 0.0397 - val_accuracy: 0.7897 - val_loss: 0.0388\n",
      "Epoch 116/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - accuracy: 0.7811 - loss: 0.0394 - val_accuracy: 0.7914 - val_loss: 0.0385\n",
      "Epoch 117/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - accuracy: 0.7824 - loss: 0.0391 - val_accuracy: 0.7927 - val_loss: 0.0383\n",
      "Epoch 118/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - accuracy: 0.7844 - loss: 0.0391 - val_accuracy: 0.7943 - val_loss: 0.0381\n",
      "Epoch 119/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - accuracy: 0.7869 - loss: 0.0387 - val_accuracy: 0.7957 - val_loss: 0.0379\n",
      "Epoch 120/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - accuracy: 0.7868 - loss: 0.0386 - val_accuracy: 0.7970 - val_loss: 0.0377\n",
      "Epoch 121/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - accuracy: 0.7895 - loss: 0.0384 - val_accuracy: 0.7987 - val_loss: 0.0374\n",
      "Epoch 122/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - accuracy: 0.7883 - loss: 0.0383 - val_accuracy: 0.7995 - val_loss: 0.0372\n",
      "Epoch 123/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - accuracy: 0.7954 - loss: 0.0377 - val_accuracy: 0.8025 - val_loss: 0.0370\n",
      "Epoch 124/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - accuracy: 0.7942 - loss: 0.0377 - val_accuracy: 0.8044 - val_loss: 0.0368\n",
      "Epoch 125/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - accuracy: 0.7959 - loss: 0.0374 - val_accuracy: 0.8060 - val_loss: 0.0366\n",
      "Epoch 126/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - accuracy: 0.7957 - loss: 0.0373 - val_accuracy: 0.8075 - val_loss: 0.0364\n",
      "Epoch 127/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - accuracy: 0.7970 - loss: 0.0372 - val_accuracy: 0.8087 - val_loss: 0.0362\n",
      "Epoch 128/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - accuracy: 0.8027 - loss: 0.0369 - val_accuracy: 0.8103 - val_loss: 0.0360\n",
      "Epoch 129/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step - accuracy: 0.8040 - loss: 0.0367 - val_accuracy: 0.8122 - val_loss: 0.0358\n",
      "Epoch 130/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - accuracy: 0.8041 - loss: 0.0365 - val_accuracy: 0.8139 - val_loss: 0.0356\n",
      "Epoch 131/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - accuracy: 0.8107 - loss: 0.0361 - val_accuracy: 0.8155 - val_loss: 0.0354\n",
      "Epoch 132/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - accuracy: 0.8113 - loss: 0.0360 - val_accuracy: 0.8174 - val_loss: 0.0352\n",
      "Epoch 133/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - accuracy: 0.8137 - loss: 0.0356 - val_accuracy: 0.8192 - val_loss: 0.0350\n",
      "Epoch 134/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - accuracy: 0.8115 - loss: 0.0357 - val_accuracy: 0.8221 - val_loss: 0.0348\n",
      "Epoch 135/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - accuracy: 0.8160 - loss: 0.0355 - val_accuracy: 0.8242 - val_loss: 0.0346\n",
      "Epoch 136/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - accuracy: 0.8180 - loss: 0.0353 - val_accuracy: 0.8255 - val_loss: 0.0344\n",
      "Epoch 137/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - accuracy: 0.8171 - loss: 0.0352 - val_accuracy: 0.8268 - val_loss: 0.0342\n",
      "Epoch 138/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - accuracy: 0.8188 - loss: 0.0349 - val_accuracy: 0.8282 - val_loss: 0.0341\n",
      "Epoch 139/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - accuracy: 0.8207 - loss: 0.0348 - val_accuracy: 0.8293 - val_loss: 0.0339\n",
      "Epoch 140/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 0.8212 - loss: 0.0347 - val_accuracy: 0.8306 - val_loss: 0.0337\n",
      "Epoch 141/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - accuracy: 0.8232 - loss: 0.0346 - val_accuracy: 0.8319 - val_loss: 0.0335\n",
      "Epoch 142/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - accuracy: 0.8249 - loss: 0.0344 - val_accuracy: 0.8334 - val_loss: 0.0333\n",
      "Epoch 143/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - accuracy: 0.8272 - loss: 0.0340 - val_accuracy: 0.8342 - val_loss: 0.0332\n",
      "Epoch 144/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - accuracy: 0.8303 - loss: 0.0338 - val_accuracy: 0.8359 - val_loss: 0.0330\n",
      "Epoch 145/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - accuracy: 0.8303 - loss: 0.0338 - val_accuracy: 0.8372 - val_loss: 0.0328\n",
      "Epoch 146/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - accuracy: 0.8314 - loss: 0.0334 - val_accuracy: 0.8389 - val_loss: 0.0326\n",
      "Epoch 147/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - accuracy: 0.8351 - loss: 0.0333 - val_accuracy: 0.8396 - val_loss: 0.0325\n",
      "Epoch 148/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - accuracy: 0.8336 - loss: 0.0334 - val_accuracy: 0.8413 - val_loss: 0.0323\n",
      "Epoch 149/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - accuracy: 0.8326 - loss: 0.0331 - val_accuracy: 0.8430 - val_loss: 0.0321\n",
      "Epoch 150/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - accuracy: 0.8331 - loss: 0.0330 - val_accuracy: 0.8442 - val_loss: 0.0320\n",
      "Epoch 151/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - accuracy: 0.8352 - loss: 0.0328 - val_accuracy: 0.8452 - val_loss: 0.0318\n",
      "Epoch 152/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - accuracy: 0.8373 - loss: 0.0326 - val_accuracy: 0.8458 - val_loss: 0.0317\n",
      "Epoch 153/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 0.8376 - loss: 0.0324 - val_accuracy: 0.8466 - val_loss: 0.0315\n",
      "Epoch 154/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - accuracy: 0.8386 - loss: 0.0323 - val_accuracy: 0.8478 - val_loss: 0.0313\n",
      "Epoch 155/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.8386 - loss: 0.0322 - val_accuracy: 0.8488 - val_loss: 0.0312\n",
      "Epoch 156/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - accuracy: 0.8431 - loss: 0.0318 - val_accuracy: 0.8497 - val_loss: 0.0310\n",
      "Epoch 157/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - accuracy: 0.8395 - loss: 0.0318 - val_accuracy: 0.8510 - val_loss: 0.0309\n",
      "Epoch 158/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - accuracy: 0.8417 - loss: 0.0317 - val_accuracy: 0.8517 - val_loss: 0.0307\n",
      "Epoch 159/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - accuracy: 0.8423 - loss: 0.0316 - val_accuracy: 0.8527 - val_loss: 0.0306\n",
      "Epoch 160/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - accuracy: 0.8435 - loss: 0.0314 - val_accuracy: 0.8530 - val_loss: 0.0304\n",
      "Epoch 161/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - accuracy: 0.8472 - loss: 0.0311 - val_accuracy: 0.8539 - val_loss: 0.0303\n",
      "Epoch 162/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - accuracy: 0.8474 - loss: 0.0309 - val_accuracy: 0.8541 - val_loss: 0.0301\n",
      "Epoch 163/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - accuracy: 0.8455 - loss: 0.0311 - val_accuracy: 0.8544 - val_loss: 0.0300\n",
      "Epoch 164/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - accuracy: 0.8474 - loss: 0.0308 - val_accuracy: 0.8548 - val_loss: 0.0299\n",
      "Epoch 165/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - accuracy: 0.8463 - loss: 0.0308 - val_accuracy: 0.8551 - val_loss: 0.0297\n",
      "Epoch 166/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - accuracy: 0.8475 - loss: 0.0306 - val_accuracy: 0.8560 - val_loss: 0.0296\n",
      "Epoch 167/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.8501 - loss: 0.0303 - val_accuracy: 0.8571 - val_loss: 0.0294\n",
      "Epoch 168/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - accuracy: 0.8506 - loss: 0.0303 - val_accuracy: 0.8576 - val_loss: 0.0293\n",
      "Epoch 169/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - accuracy: 0.8513 - loss: 0.0301 - val_accuracy: 0.8581 - val_loss: 0.0292\n",
      "Epoch 170/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - accuracy: 0.8532 - loss: 0.0300 - val_accuracy: 0.8586 - val_loss: 0.0290\n",
      "Epoch 171/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.8513 - loss: 0.0299 - val_accuracy: 0.8594 - val_loss: 0.0289\n",
      "Epoch 172/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - accuracy: 0.8524 - loss: 0.0299 - val_accuracy: 0.8594 - val_loss: 0.0288\n",
      "Epoch 173/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - accuracy: 0.8502 - loss: 0.0298 - val_accuracy: 0.8601 - val_loss: 0.0287\n",
      "Epoch 174/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - accuracy: 0.8542 - loss: 0.0295 - val_accuracy: 0.8605 - val_loss: 0.0285\n",
      "Epoch 175/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - accuracy: 0.8552 - loss: 0.0292 - val_accuracy: 0.8609 - val_loss: 0.0284\n",
      "Epoch 176/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - accuracy: 0.8515 - loss: 0.0294 - val_accuracy: 0.8612 - val_loss: 0.0283\n",
      "Epoch 177/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 0.8531 - loss: 0.0294 - val_accuracy: 0.8618 - val_loss: 0.0282\n",
      "Epoch 178/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - accuracy: 0.8553 - loss: 0.0290 - val_accuracy: 0.8623 - val_loss: 0.0281\n",
      "Epoch 179/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 0.8564 - loss: 0.0288 - val_accuracy: 0.8630 - val_loss: 0.0279\n",
      "Epoch 180/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8555 - loss: 0.0289 - val_accuracy: 0.8635 - val_loss: 0.0278\n",
      "Epoch 181/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step - accuracy: 0.8550 - loss: 0.0288 - val_accuracy: 0.8640 - val_loss: 0.0277\n",
      "Epoch 182/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - accuracy: 0.8556 - loss: 0.0286 - val_accuracy: 0.8648 - val_loss: 0.0276\n",
      "Epoch 183/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899us/step - accuracy: 0.8577 - loss: 0.0284 - val_accuracy: 0.8654 - val_loss: 0.0275\n",
      "Epoch 184/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 950us/step - accuracy: 0.8585 - loss: 0.0283 - val_accuracy: 0.8658 - val_loss: 0.0274\n",
      "Epoch 185/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8584 - loss: 0.0281 - val_accuracy: 0.8665 - val_loss: 0.0273\n",
      "Epoch 186/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8588 - loss: 0.0281 - val_accuracy: 0.8672 - val_loss: 0.0271\n",
      "Epoch 187/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899us/step - accuracy: 0.8588 - loss: 0.0281 - val_accuracy: 0.8677 - val_loss: 0.0270\n",
      "Epoch 188/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - accuracy: 0.8620 - loss: 0.0276 - val_accuracy: 0.8683 - val_loss: 0.0269\n",
      "Epoch 189/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - accuracy: 0.8611 - loss: 0.0277 - val_accuracy: 0.8691 - val_loss: 0.0268\n",
      "Epoch 190/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - accuracy: 0.8606 - loss: 0.0276 - val_accuracy: 0.8693 - val_loss: 0.0267\n",
      "Epoch 191/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - accuracy: 0.8619 - loss: 0.0274 - val_accuracy: 0.8695 - val_loss: 0.0266\n",
      "Epoch 192/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - accuracy: 0.8608 - loss: 0.0274 - val_accuracy: 0.8699 - val_loss: 0.0265\n",
      "Epoch 193/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - accuracy: 0.8580 - loss: 0.0277 - val_accuracy: 0.8698 - val_loss: 0.0264\n",
      "Epoch 194/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - accuracy: 0.8629 - loss: 0.0270 - val_accuracy: 0.8700 - val_loss: 0.0263\n",
      "Epoch 195/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - accuracy: 0.8606 - loss: 0.0272 - val_accuracy: 0.8704 - val_loss: 0.0262\n",
      "Epoch 196/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - accuracy: 0.8613 - loss: 0.0270 - val_accuracy: 0.8707 - val_loss: 0.0261\n",
      "Epoch 197/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - accuracy: 0.8652 - loss: 0.0267 - val_accuracy: 0.8709 - val_loss: 0.0260\n",
      "Epoch 198/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - accuracy: 0.8636 - loss: 0.0267 - val_accuracy: 0.8713 - val_loss: 0.0259\n",
      "Epoch 199/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - accuracy: 0.8661 - loss: 0.0266 - val_accuracy: 0.8717 - val_loss: 0.0258\n",
      "Epoch 200/200\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - accuracy: 0.8649 - loss: 0.0265 - val_accuracy: 0.8717 - val_loss: 0.0257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x31e7c6180>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.api.optimizers import SGD\n",
    "\n",
    "simple_model.compile(\n",
    "    loss=\"mean_squared_error\",\n",
    "    optimizer=SGD(learning_rate=0.01),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "simple_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=200,\n",
    "    validation_data=(X_valid, y_valid),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20501a84",
   "metadata": {},
   "source": [
    "#### Оценка качества простой модели\n",
    "\n",
    "Лучшее качество модели: 87.17 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30db9b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step - accuracy: 0.8537 - loss: 0.0286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.02574083022773266, 0.8716999888420105]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db127c8",
   "metadata": {},
   "source": [
    "#### Проектирование архитектуры более сложной ИНС\n",
    "\n",
    "Добавлен дополнительный скрытый полносвязный слой\n",
    "\n",
    "Все скрытые слои используют ReLU-нейроны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2f2c585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,050</span> (215.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m55,050\u001b[0m (215.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,050</span> (215.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m55,050\u001b[0m (215.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "difficult_model = Sequential()\n",
    "difficult_model.add(InputLayer(shape=(28 * 28,)))\n",
    "difficult_model.add(Dense(64, activation=\"relu\"))\n",
    "difficult_model.add(Dense(64, activation=\"relu\"))\n",
    "difficult_model.add(Dense(10, activation=\"softmax\"))\n",
    "difficult_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5650eddc",
   "metadata": {},
   "source": [
    "#### Обучение более сложной модели\n",
    "\n",
    "Функция стоимости изменена на перекрестную энтропию (лучше подходит для классификации)\n",
    "\n",
    "Количество эпох уменьшено с 200 до 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10fc0413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7675 - loss: 0.7946 - val_accuracy: 0.9248 - val_loss: 0.2552\n",
      "Epoch 2/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - accuracy: 0.9283 - loss: 0.2473 - val_accuracy: 0.9418 - val_loss: 0.1918\n",
      "Epoch 3/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - accuracy: 0.9464 - loss: 0.1817 - val_accuracy: 0.9496 - val_loss: 0.1634\n",
      "Epoch 4/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - accuracy: 0.9560 - loss: 0.1483 - val_accuracy: 0.9599 - val_loss: 0.1340\n",
      "Epoch 5/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - accuracy: 0.9641 - loss: 0.1242 - val_accuracy: 0.9621 - val_loss: 0.1248\n",
      "Epoch 6/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - accuracy: 0.9672 - loss: 0.1133 - val_accuracy: 0.9643 - val_loss: 0.1167\n",
      "Epoch 7/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - accuracy: 0.9719 - loss: 0.0949 - val_accuracy: 0.9654 - val_loss: 0.1107\n",
      "Epoch 8/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - accuracy: 0.9731 - loss: 0.0878 - val_accuracy: 0.9675 - val_loss: 0.1058\n",
      "Epoch 9/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - accuracy: 0.9757 - loss: 0.0789 - val_accuracy: 0.9669 - val_loss: 0.1066\n",
      "Epoch 10/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - accuracy: 0.9773 - loss: 0.0777 - val_accuracy: 0.9670 - val_loss: 0.1055\n",
      "Epoch 11/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - accuracy: 0.9795 - loss: 0.0689 - val_accuracy: 0.9691 - val_loss: 0.0971\n",
      "Epoch 12/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - accuracy: 0.9823 - loss: 0.0608 - val_accuracy: 0.9702 - val_loss: 0.0933\n",
      "Epoch 13/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - accuracy: 0.9841 - loss: 0.0553 - val_accuracy: 0.9710 - val_loss: 0.0899\n",
      "Epoch 14/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - accuracy: 0.9853 - loss: 0.0496 - val_accuracy: 0.9731 - val_loss: 0.0848\n",
      "Epoch 15/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - accuracy: 0.9859 - loss: 0.0474 - val_accuracy: 0.9720 - val_loss: 0.0918\n",
      "Epoch 16/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - accuracy: 0.9870 - loss: 0.0454 - val_accuracy: 0.9716 - val_loss: 0.0902\n",
      "Epoch 17/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - accuracy: 0.9878 - loss: 0.0415 - val_accuracy: 0.9733 - val_loss: 0.0858\n",
      "Epoch 18/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - accuracy: 0.9886 - loss: 0.0381 - val_accuracy: 0.9712 - val_loss: 0.0928\n",
      "Epoch 19/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - accuracy: 0.9905 - loss: 0.0344 - val_accuracy: 0.9746 - val_loss: 0.0849\n",
      "Epoch 20/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - accuracy: 0.9907 - loss: 0.0330 - val_accuracy: 0.9731 - val_loss: 0.0890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x177e320f0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difficult_model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=SGD(learning_rate=0.1),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "difficult_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    validation_data=(X_valid, y_valid),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef2db19",
   "metadata": {},
   "source": [
    "#### Оценка качества более сложной модели\n",
    "\n",
    "Лучшее качество модели: 97.31 %\n",
    "\n",
    "При этом количество эпох обучения значительно сократилось (с 200 до 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df92dc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9691 - loss: 0.1005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08897283673286438, 0.9731000065803528]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difficult_model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768404ed",
   "metadata": {},
   "source": [
    "#### Проектирование архитектуры глубокой ИНС\n",
    "\n",
    "В ИНС теперь три скрытых полносвязных слоя с ReLU-нейронами\n",
    "\n",
    "Для выходов каждого скрытого слоя используется пакетная нормализация\n",
    "\n",
    "Для последнего скрытого слоя применяется прореживание, при котором отключается 20 % случайных нейронов\n",
    "\n",
    "Keras автоматически корректирует значения (умножает входы на 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddbbb225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m50,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">59,978</span> (234.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m59,978\u001b[0m (234.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">59,594</span> (232.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m59,594\u001b[0m (232.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.api.layers import Dropout\n",
    "from keras.api.layers import BatchNormalization\n",
    "\n",
    "deep_model = Sequential()\n",
    "deep_model.add(InputLayer(shape=(28 * 28,)))\n",
    "deep_model.add(Dense(64, activation=\"relu\"))\n",
    "deep_model.add(BatchNormalization())\n",
    "deep_model.add(Dense(64, activation=\"relu\"))\n",
    "deep_model.add(BatchNormalization())\n",
    "deep_model.add(Dense(64, activation=\"relu\"))\n",
    "deep_model.add(BatchNormalization())\n",
    "deep_model.add(Dropout(0.2))\n",
    "deep_model.add(Dense(10, activation=\"softmax\"))\n",
    "deep_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985823c3",
   "metadata": {},
   "source": [
    "#### Обучение глубокой модели\n",
    "\n",
    "Вместо SGD используется оптимизатор Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02f0f967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7918 - loss: 0.6796 - val_accuracy: 0.9486 - val_loss: 0.1685\n",
      "Epoch 2/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9504 - loss: 0.1657 - val_accuracy: 0.9623 - val_loss: 0.1202\n",
      "Epoch 3/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9639 - loss: 0.1194 - val_accuracy: 0.9688 - val_loss: 0.0993\n",
      "Epoch 4/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9716 - loss: 0.0908 - val_accuracy: 0.9703 - val_loss: 0.0983\n",
      "Epoch 5/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9745 - loss: 0.0797 - val_accuracy: 0.9706 - val_loss: 0.0998\n",
      "Epoch 6/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9801 - loss: 0.0656 - val_accuracy: 0.9723 - val_loss: 0.0935\n",
      "Epoch 7/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9814 - loss: 0.0581 - val_accuracy: 0.9712 - val_loss: 0.0963\n",
      "Epoch 8/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9832 - loss: 0.0509 - val_accuracy: 0.9697 - val_loss: 0.1091\n",
      "Epoch 9/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9858 - loss: 0.0449 - val_accuracy: 0.9739 - val_loss: 0.0908\n",
      "Epoch 10/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9864 - loss: 0.0418 - val_accuracy: 0.9708 - val_loss: 0.1073\n",
      "Epoch 11/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9881 - loss: 0.0358 - val_accuracy: 0.9767 - val_loss: 0.0874\n",
      "Epoch 12/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9881 - loss: 0.0358 - val_accuracy: 0.9738 - val_loss: 0.0973\n",
      "Epoch 13/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9896 - loss: 0.0323 - val_accuracy: 0.9744 - val_loss: 0.0942\n",
      "Epoch 14/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9899 - loss: 0.0287 - val_accuracy: 0.9725 - val_loss: 0.0970\n",
      "Epoch 15/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9917 - loss: 0.0261 - val_accuracy: 0.9726 - val_loss: 0.1056\n",
      "Epoch 16/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9898 - loss: 0.0289 - val_accuracy: 0.9742 - val_loss: 0.0959\n",
      "Epoch 17/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9925 - loss: 0.0229 - val_accuracy: 0.9757 - val_loss: 0.0980\n",
      "Epoch 18/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9926 - loss: 0.0225 - val_accuracy: 0.9716 - val_loss: 0.1078\n",
      "Epoch 19/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9926 - loss: 0.0215 - val_accuracy: 0.9776 - val_loss: 0.0906\n",
      "Epoch 20/20\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9938 - loss: 0.0191 - val_accuracy: 0.9746 - val_loss: 0.1025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x31c896a50>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "deep_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    validation_data=(X_valid, y_valid),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc87962",
   "metadata": {},
   "source": [
    "#### Оценка качества глубокой модели\n",
    "\n",
    "Лучшее качество модели: 97.46 %\n",
    "\n",
    "Качество модели незначительно улучшилось за счет улучшения архитектуры сети и смены оптимизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70d626e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9713 - loss: 0.1168 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10245733708143234, 0.9746000170707703]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc09e059",
   "metadata": {},
   "source": [
    "### Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cfb72f",
   "metadata": {},
   "source": [
    "#### Загрузка данных для задачи регрессии\n",
    "\n",
    "Набор данных о жилье в Бостоне собран Службой переписи населения США.\n",
    "\n",
    "Входные признаки:\n",
    "- CRIM — уровень преступности на душу населения по районам;\n",
    "- ZN — доля жилых земель, отведенных под участки площадью более 25 000 кв. футов;\n",
    "- INDUS — доля неторговых акров в городе;\n",
    "- CHAS — 1, если участок граничит с рекой; 0 в противном случае;\n",
    "- NOX — концентрация оксидов азота;\n",
    "- RM — среднее количество комнат в помещении;\n",
    "- AGE — доля домов, построенных до 1940 года;\n",
    "- DIS — взвешенные расстояния до пяти центров занятости Бостона;\n",
    "- RAD — индекс доступности радиальных автомагистралей;\n",
    "- TAX — ставка налога на имущество на полную стоимость;\n",
    "- PTRATIO — соотношение учеников и учителей по районам;\n",
    "- B — доля чернокожих по районам;\n",
    "- LSTAT — % населения с более низким статусом.\n",
    "\n",
    "Целевой признак:\n",
    "- MEDV — медианная стоимость домов в тысячах долларов США.\n",
    "\n",
    "Данные уже предобработаны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23146b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "\u001b[1m57026/57026\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.23247e+00, 0.00000e+00, 8.14000e+00, ..., 2.10000e+01,\n",
       "        3.96900e+02, 1.87200e+01],\n",
       "       [2.17700e-02, 8.25000e+01, 2.03000e+00, ..., 1.47000e+01,\n",
       "        3.95380e+02, 3.11000e+00],\n",
       "       [4.89822e+00, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
       "        3.75520e+02, 3.26000e+00],\n",
       "       ...,\n",
       "       [3.46600e-02, 3.50000e+01, 6.06000e+00, ..., 1.69000e+01,\n",
       "        3.62250e+02, 7.83000e+00],\n",
       "       [2.14918e+00, 0.00000e+00, 1.95800e+01, ..., 1.47000e+01,\n",
       "        2.61950e+02, 1.57900e+01],\n",
       "       [1.43900e-02, 6.00000e+01, 2.93000e+00, ..., 1.56000e+01,\n",
       "        3.76700e+02, 4.38000e+00]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1,\n",
       "       17.9, 23.1, 19.9, 15.7,  8.8, 50. , 22.5, 24.1, 27.5, 10.9, 30.8,\n",
       "       32.9, 24. , 18.5, 13.3, 22.9, 34.7, 16.6, 17.5, 22.3, 16.1, 14.9,\n",
       "       23.1, 34.9, 25. , 13.9, 13.1, 20.4, 20. , 15.2, 24.7, 22.2, 16.7,\n",
       "       12.7, 15.6, 18.4, 21. , 30.1, 15.1, 18.7,  9.6, 31.5, 24.8, 19.1,\n",
       "       22. , 14.5, 11. , 32. , 29.4, 20.3, 24.4, 14.6, 19.5, 14.1, 14.3,\n",
       "       15.6, 10.5,  6.3, 19.3, 19.3, 13.4, 36.4, 17.8, 13.5, 16.5,  8.3,\n",
       "       14.3, 16. , 13.4, 28.6, 43.5, 20.2, 22. , 23. , 20.7, 12.5, 48.5,\n",
       "       14.6, 13.4, 23.7, 50. , 21.7, 39.8, 38.7, 22.2, 34.9, 22.5, 31.1,\n",
       "       28.7, 46. , 41.7, 21. , 26.6, 15. , 24.4, 13.3, 21.2, 11.7, 21.7,\n",
       "       19.4, 50. , 22.8, 19.7, 24.7, 36.2, 14.2, 18.9, 18.3, 20.6, 24.6,\n",
       "       18.2,  8.7, 44. , 10.4, 13.2, 21.2, 37. , 30.7, 22.9, 20. , 19.3,\n",
       "       31.7, 32. , 23.1, 18.8, 10.9, 50. , 19.6,  5. , 14.4, 19.8, 13.8,\n",
       "       19.6, 23.9, 24.5, 25. , 19.9, 17.2, 24.6, 13.5, 26.6, 21.4, 11.9,\n",
       "       22.6, 19.6,  8.5, 23.7, 23.1, 22.4, 20.5, 23.6, 18.4, 35.2, 23.1,\n",
       "       27.9, 20.6, 23.7, 28. , 13.6, 27.1, 23.6, 20.6, 18.2, 21.7, 17.1,\n",
       "        8.4, 25.3, 13.8, 22.2, 18.4, 20.7, 31.6, 30.5, 20.3,  8.8, 19.2,\n",
       "       19.4, 23.1, 23. , 14.8, 48.8, 22.6, 33.4, 21.1, 13.6, 32.2, 13.1,\n",
       "       23.4, 18.9, 23.9, 11.8, 23.3, 22.8, 19.6, 16.7, 13.4, 22.2, 20.4,\n",
       "       21.8, 26.4, 14.9, 24.1, 23.8, 12.3, 29.1, 21. , 19.5, 23.3, 23.8,\n",
       "       17.8, 11.5, 21.7, 19.9, 25. , 33.4, 28.5, 21.4, 24.3, 27.5, 33.1,\n",
       "       16.2, 23.3, 48.3, 22.9, 22.8, 13.1, 12.7, 22.6, 15. , 15.3, 10.5,\n",
       "       24. , 18.5, 21.7, 19.5, 33.2, 23.2,  5. , 19.1, 12.7, 22.3, 10.2,\n",
       "       13.9, 16.3, 17. , 20.1, 29.9, 17.2, 37.3, 45.4, 17.8, 23.2, 29. ,\n",
       "       22. , 18. , 17.4, 34.6, 20.1, 25. , 15.6, 24.8, 28.2, 21.2, 21.4,\n",
       "       23.8, 31. , 26.2, 17.4, 37.9, 17.5, 20. ,  8.3, 23.9,  8.4, 13.8,\n",
       "        7.2, 11.7, 17.1, 21.6, 50. , 16.1, 20.4, 20.6, 21.4, 20.6, 36.5,\n",
       "        8.5, 24.8, 10.8, 21.9, 17.3, 18.9, 36.2, 14.9, 18.2, 33.3, 21.8,\n",
       "       19.7, 31.6, 24.8, 19.4, 22.8,  7.5, 44.8, 16.8, 18.7, 50. , 50. ,\n",
       "       19.5, 20.1, 50. , 17.2, 20.8, 19.3, 41.3, 20.4, 20.5, 13.8, 16.5,\n",
       "       23.9, 20.6, 31.5, 23.3, 16.8, 14. , 33.8, 36.1, 12.8, 18.3, 18.7,\n",
       "       19.1, 29. , 30.1, 50. , 50. , 22. , 11.9, 37.6, 50. , 22.7, 20.8,\n",
       "       23.5, 27.9, 50. , 19.3, 23.9, 22.6, 15.2, 21.7, 19.2, 43.8, 20.3,\n",
       "       33.2, 19.9, 22.5, 32.7, 22. , 17.1, 19. , 15. , 16.1, 25.1, 23.7,\n",
       "       28.7, 37.2, 22.6, 16.4, 25. , 29.8, 22.1, 17.4, 18.1, 30.3, 17.5,\n",
       "       24.7, 12.6, 26.5, 28.7, 13.3, 10.4, 24.4, 23. , 20. , 17.8,  7. ,\n",
       "       11.8, 24.4, 13.8, 19.4, 25.2, 19.4, 19.4, 29.1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.api.datasets import boston_housing\n",
    "\n",
    "(X_train, y_train), (X_valid, y_valid) = boston_housing.load_data()\n",
    "\n",
    "display(X_train)\n",
    "display(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf59b0ae",
   "metadata": {},
   "source": [
    "#### Проектирование ИНС для задачи регрессии\n",
    "\n",
    "Для решения задачи регрессии в выходном слое используются нейроны с линейной функцией активации\n",
    "\n",
    "Создавать более сложную архитектуру не имеет смысла, так как в наборе данных мало признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ebaf129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,185</span> (4.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,185\u001b[0m (4.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,089</span> (4.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,089\u001b[0m (4.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> (384.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m96\u001b[0m (384.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg_model = Sequential()\n",
    "reg_model.add(InputLayer(shape=(13,)))\n",
    "reg_model.add(Dense(32, activation=\"relu\"))\n",
    "reg_model.add(BatchNormalization())\n",
    "reg_model.add(Dense(16, activation=\"relu\"))\n",
    "reg_model.add(BatchNormalization())\n",
    "reg_model.add(Dropout(0.2))\n",
    "reg_model.add(Dense(1, activation=\"linear\"))\n",
    "reg_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474627ee",
   "metadata": {},
   "source": [
    "#### Обучение модели для регрессии\n",
    "\n",
    "Функция стоимости: MSE (лучше подходит для задачи регрессии)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b1a0038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 29.1118 - val_loss: 46.1019\n",
      "Epoch 2/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step - loss: 35.6149 - val_loss: 130.9010\n",
      "Epoch 3/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - loss: 33.8576 - val_loss: 99.9658\n",
      "Epoch 4/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 36.8302 - val_loss: 784.9380\n",
      "Epoch 5/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - loss: 32.9512 - val_loss: 242.2975\n",
      "Epoch 6/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 40.3341 - val_loss: 111.1783\n",
      "Epoch 7/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - loss: 37.1001 - val_loss: 110.1724\n",
      "Epoch 8/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - loss: 38.9198 - val_loss: 154.4365\n",
      "Epoch 9/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341us/step - loss: 34.3625 - val_loss: 109.0682\n",
      "Epoch 10/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378us/step - loss: 28.0040 - val_loss: 200.6869\n",
      "Epoch 11/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - loss: 37.9714 - val_loss: 144.2244\n",
      "Epoch 12/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359us/step - loss: 32.7022 - val_loss: 75.9799\n",
      "Epoch 13/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - loss: 40.9319 - val_loss: 53.7583\n",
      "Epoch 14/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step - loss: 37.0839 - val_loss: 65.8641\n",
      "Epoch 15/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - loss: 35.6123 - val_loss: 67.8268\n",
      "Epoch 16/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - loss: 36.7169 - val_loss: 142.6284\n",
      "Epoch 17/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341us/step - loss: 34.8339 - val_loss: 35.0198\n",
      "Epoch 18/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - loss: 29.8132 - val_loss: 74.8143\n",
      "Epoch 19/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - loss: 36.6735 - val_loss: 126.3095\n",
      "Epoch 20/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - loss: 31.1710 - val_loss: 77.3345\n",
      "Epoch 21/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 31.3717 - val_loss: 46.9382\n",
      "Epoch 22/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - loss: 39.6129 - val_loss: 45.6828\n",
      "Epoch 23/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392us/step - loss: 32.4640 - val_loss: 42.9273\n",
      "Epoch 24/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344us/step - loss: 43.6678 - val_loss: 45.0138\n",
      "Epoch 25/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 31.3969 - val_loss: 36.8469\n",
      "Epoch 26/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 40.7973 - val_loss: 53.8602\n",
      "Epoch 27/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 43.2745 - val_loss: 48.1137\n",
      "Epoch 28/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - loss: 29.5495 - val_loss: 135.0972\n",
      "Epoch 29/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398us/step - loss: 40.1542 - val_loss: 56.0914\n",
      "Epoch 30/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390us/step - loss: 36.2470 - val_loss: 34.4832\n",
      "Epoch 31/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400us/step - loss: 45.1757 - val_loss: 36.8885\n",
      "Epoch 32/32\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 35.5981 - val_loss: 29.2268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1079c80b0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_model.compile(\n",
    "    loss=\"mean_squared_error\",\n",
    "    optimizer=\"adam\",\n",
    ")\n",
    "\n",
    "reg_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=8,\n",
    "    epochs=32,\n",
    "    validation_data=(X_valid, y_valid),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb962af9",
   "metadata": {},
   "source": [
    "#### Оценка качества модели для регрессии\n",
    "\n",
    "Средняя ошибка на тестовой выборке: 29.22 тысячи долларов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "727e9df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.069733"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "14.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_hat = reg_model.predict(np.reshape(X_valid[42], [1, 13]))\n",
    "display(y_hat[0][0])\n",
    "display(y_valid[42])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
